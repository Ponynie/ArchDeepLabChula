{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgwWsYMFp1bC"
      },
      "source": [
        "### 1. Multiple Regression\n",
        "\n",
        "Consider energy efficiency data  UCI repository which is supplied by Athanasios Tsanas and Angeliki Xifara. It contains 10 attributes :\n",
        "1. X1 - Relative Compactness : continuous data with 0.62 to 0.98\n",
        "2. X2 - Surface Area : continuous data with 514.50 to 808.50\n",
        "3. X3 - Wall Area : continuous data with 245.00 to 416.50\n",
        "4. X4 - Roof Area : continuous data with 110.25 to 220.50\n",
        "5. X5 - Overall Height : continuous data with 3.50 to 7.00\n",
        "6. X6 - Orientation : integer data with 2.00 to 5.00\n",
        "7. X7 - Glazing Area : continuous data with 0.00 to 0.40\n",
        "8. X8 - Glazing Area Distribution : integer data with 0.00 to 5.00\n",
        "9. Y1 - Heating Load : continuous data with 6.01 to 43.10\n",
        "10. Y2 - Cooling Load : continuous data with 10.90 to 48.03\n",
        "\n",
        "\n",
        "The target is the $9^{th}$ attribute, the response variable Y1 and do not use Y2 in the model (but if you want to use, you can try).\n",
        "\n",
        "There is no missing values.\n",
        "\n",
        "You must implement the multiple regression using **SGD** from our class with average square loss.\n",
        "- The total 10 points will give to a student who can find the parameters of this regression and achieve 0.01 for the average square loss.\n",
        "- The total 7 points will give to a student who can find the parameters of this regression and achieve 0.1 for the average square loss.\n",
        "- The total 5 points will give to a student who can make this code work with this dataset.\n",
        "\n",
        "This data addresses in https://archive.ics.uci.edu/static/public/242/energy+efficiency.zip . Hint : you have to use !wget and !unzip this data from !wget and you can use pandas library and sklearn.preprocessing  to deal with data. TA's average square loss from last epoch is around 0.006. I think you can do better than me. Cheer up!! you can do this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sULiNGnnYTNt"
      },
      "outputs": [],
      "source": [
        "# Must run this statement first\n",
        "!pip install --quiet mxnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4xpViNL3dDsX"
      },
      "outputs": [],
      "source": [
        "# We will need to process this text dataset using pandas\n",
        "# Deal with data in this block\n",
        "import mxnet as mx\n",
        "from mxnet import nd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# !wget https://archive.ics.uci.edu/static/public/242/energy+efficiency.zip\n",
        "# !unzip ./energy+efficiency.zip\n",
        "\n",
        "df = pd.read_excel('ENB2012_data.xlsx')\n",
        "\n",
        "df.drop([\"Y2\"], axis=1, inplace=True)\n",
        "\n",
        "# df.head(10)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "\n",
        "# df.describe()\n",
        "\n",
        "min_max_scaler = MinMaxScaler()\n",
        "df_normalized = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "target = nd.array(df_normalized[\"Y1\"])\n",
        "features = nd.array(df_normalized.drop(\"Y1\", axis=1))\n",
        "\n",
        "# print(target.shape, features.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udEf-zyzgDql",
        "outputId": "edc7a37c-eb3d-4e2b-ecbc-6cff9c2b925b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1, loss: [0.01802309]\n",
            "epoch 2, loss: [0.04259727]\n",
            "epoch 3, loss: [0.02673271]\n",
            "epoch 4, loss: [0.00929978]\n",
            "epoch 5, loss: [0.02260777]\n",
            "epoch 6, loss: [0.01039464]\n",
            "epoch 7, loss: [0.00400636]\n",
            "epoch 8, loss: [0.00856073]\n",
            "epoch 9, loss: [0.00666468]\n",
            "epoch 10, loss: [0.00874935]\n",
            "epoch 11, loss: [0.0044263]\n",
            "epoch 12, loss: [0.00520474]\n",
            "epoch 13, loss: [0.00377015]\n",
            "epoch 14, loss: [0.01466225]\n",
            "epoch 15, loss: [0.00347824]\n",
            "epoch 16, loss: [0.00300786]\n",
            "epoch 17, loss: [0.00235405]\n",
            "epoch 18, loss: [0.01052281]\n",
            "epoch 19, loss: [0.00951543]\n",
            "epoch 20, loss: [0.00189464]\n",
            "Model's Weight:\n",
            " [[ 1.13266625e-01  1.10821944e-04  1.61712468e-01 -1.04915150e-01\n",
            "   2.98812062e-01  1.50882713e-02  1.46970242e-01  5.92767149e-02]]\n",
            "Model's Bias:\n",
            " [0.11378725]\n"
          ]
        }
      ],
      "source": [
        "# Build multiple regression from ground-up\n",
        "import mxnet as mx\n",
        "from mxnet import nd, autograd, gluon\n",
        "\n",
        "# Hyper parameter\n",
        "batch_size = 10\n",
        "lr = 0.005\n",
        "epochs = 20\n",
        "\n",
        "trainset = gluon.data.ArrayDataset(features, target)\n",
        "trainloader = gluon.data.DataLoader(trainset, batch_size, shuffle=True)\n",
        "\n",
        "from mxnet.gluon import nn\n",
        "from mxnet import init\n",
        "\n",
        "model = nn.Sequential()\n",
        "model.add(nn.Dense(1))\n",
        "model.initialize(init.Normal(sigma=0.01))\n",
        "\n",
        "criterion = gluon.loss.L2Loss()\n",
        "optimizer = gluon.Trainer(model.collect_params(), 'sgd', {'learning_rate': lr})\n",
        "\n",
        "# Traning Loop\n",
        "for epoch in range(epochs):\n",
        "  for features, target in trainloader:\n",
        "    with autograd.record():\n",
        "      logits = model(features)\n",
        "      loss = criterion(logits, target)\n",
        "    loss.backward()\n",
        "    optimizer.step(batch_size)\n",
        "  loss = criterion(model(features), target)\n",
        "  print(f'epoch {epoch + 1}, loss: {loss.mean().asnumpy()}')\n",
        "\n",
        "\n",
        "# Model parameters\n",
        "weight = model[0].weight.data().asnumpy()\n",
        "bias = model[0].bias.data().asnumpy()\n",
        "\n",
        "print(f\"Model's Weight:\\n {weight}\")\n",
        "print(f\"Model's Bias:\\n {bias}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
